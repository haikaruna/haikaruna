{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN7S27zpveLh3frijCf7yDZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haikaruna/haikaruna/blob/main/Pyspark_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AYc3RNiOTCp"
      },
      "outputs": [],
      "source": [
        "import pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oi2vBDVPRG8",
        "outputId": "0878ad0f-d787-4026-a94b-87078992d15e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 43 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 74.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=af5b69636c888d43d828a8779e4b7d944b3e4bd7d055e7f6178d595c118b337b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "                    .appName('SparkByExamples.com') \\\n",
        "                    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "EsiF84cjOWS_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "62b9b23f-21cc-4314-d5c7-44ab72901bf5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-42306a8e8add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[1]\"\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SparkByExamples.com'\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n18ZmxaTOhxG",
        "outputId": "a7ddaf88-49f2-4b20-9e26-495381a0bd10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_rdd=sc.parallelize(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "G_DmwA2hOwie",
        "outputId": "587167f4-fa8f-41c1-ac1d-ede711e074f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-19e8480f9bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_rdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q0_QvnUPwp2",
        "outputId": "27b8aac9-21d2-4d0b-e479-eaa538b86618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.json(\"/content/sample_data/anscombe.json\")"
      ],
      "metadata": {
        "id": "lT_PKmilP-HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ6YigCPQDdV",
        "outputId": "e507a3d6-e981-4b8c-c311-542802258a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+-----+---------------+\n",
            "|Series|   X|    Y|_corrupt_record|\n",
            "+------+----+-----+---------------+\n",
            "|  null|null| null|              [|\n",
            "|     I|10.0| 8.04|           null|\n",
            "|     I| 8.0| 6.95|           null|\n",
            "|     I|13.0| 7.58|           null|\n",
            "|     I| 9.0| 8.81|           null|\n",
            "|     I|11.0| 8.33|           null|\n",
            "|     I|14.0| 9.96|           null|\n",
            "|     I| 6.0| 7.24|           null|\n",
            "|     I| 4.0| 4.26|           null|\n",
            "|     I|12.0|10.84|           null|\n",
            "|     I| 7.0| 4.81|           null|\n",
            "|     I| 5.0| 5.68|           null|\n",
            "+------+----+-----+---------------+\n",
            "only showing top 12 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import request"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "rCcEZdSlQHJ-",
        "outputId": "ca8f0d57-21b1-4a32-ddfa-2867e457038c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-e392607f0c42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'request'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,10):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNCJYZzzQ5uD",
        "outputId": "9d32ecd3-b96b-4f2b-97c0-f0f488065853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(range(0,10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAKzjy4GQgHt",
        "outputId": "4d3caa36-ea85-4dbf-faa0-c8d7017c5b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p=list(range(1,10))"
      ],
      "metadata": {
        "id": "U4heGkv5RyAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkAu-tb1R_TN",
        "outputId": "b5c7badc-882d-4aaf-acb7-cf6f81c696b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install koalas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS48qjKuSAAt",
        "outputId": "5d1dcc51-49a0-407c-8a61-d451b0731111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting koalas\n",
            "  Downloading koalas-1.8.2-py3-none-any.whl (390 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 30 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 40 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 51 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 61 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 71 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 81 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 92 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 102 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 112 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 122 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 133 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 143 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 153 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 163 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 174 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 184 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 194 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 204 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 215 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 225 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 235 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 245 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 256 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 266 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 276 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 286 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 296 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 307 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 317 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 327 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 337 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 348 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 358 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 368 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 378 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 389 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 390 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.7/dist-packages (from koalas) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.23.2 in /usr/local/lib/python3.7/dist-packages (from koalas) (1.1.5)\n",
            "Requirement already satisfied: pyarrow>=0.10 in /usr/local/lib/python3.7/dist-packages (from koalas) (3.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.2->koalas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.2->koalas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.2->koalas) (1.15.0)\n",
            "Installing collected packages: koalas\n",
            "Successfully installed koalas-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import databricks.koalas as ks\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDWNAqb7h8mK",
        "outputId": "2546a3eb-27fd-419a-dce9-cf2b01b6b50f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Found pyspark version \"3.2.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "                    .appName('SparkByExamples.com') \\\n",
        "                    .getOrCreate()"
      ],
      "metadata": {
        "id": "lGI-iAYUiRXq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simpleData = [(\"James\", \"Sales\", 3000),\n",
        "    (\"Michael\", \"Sales\", 4600),\n",
        "    (\"Robert\", \"Sales\", 4100),\n",
        "    (\"Maria\", \"Finance\", 3000),\n",
        "    (\"James\", \"Sales\", 3000),\n",
        "    (\"Scott\", \"Finance\", 3300),\n",
        "    (\"Jen\", \"Finance\", 3900),\n",
        "    (\"Jeff\", \"Marketing\", 3000),\n",
        "    (\"Kumar\", \"Marketing\", 2000),\n",
        "    (\"Saif\", \"Sales\", 4100)\n",
        "  ]\n",
        "schema = [\"employee_name\", \"department\", \"salary\"]"
      ],
      "metadata": {
        "id": "37OTbfutiZvZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCoZRlPn1X7a",
        "outputId": "bda57e7e-4f85-4826-ca7e-0fcc9845aa9b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
        "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
        "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
        "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
        "from pyspark.sql.functions import variance,var_samp,  var_pop"
      ],
      "metadata": {
        "id": "s5v3fCoD1bvj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff=df.select(collect_list(\"salary\"))\n",
        "\n",
        "dfff=df.select(collect_set(\"salary\"))"
      ],
      "metadata": {
        "id": "0CpopYMd1hNA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiXX7d0P1jtU",
        "outputId": "73dd42bb-97c7-4e76-83e8-259ae31ddc38"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------+\n",
            "|collect_list(salary)                                        |\n",
            "+------------------------------------------------------------+\n",
            "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
            "+------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfff.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md_6jXIn1rxM",
        "outputId": "b3a783b0-ff9a-44ab-ec76-12d71d2e4552"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------+\n",
            "|collect_set(salary)                 |\n",
            "+------------------------------------+\n",
            "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
            "+------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(sum(\"salary\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSXCIQn_1t4a",
        "outputId": "a1f1dffa-00da-4e9c-f140-282af9f22827"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|sum(salary)|\n",
            "+-----------+\n",
            "|34000      |\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
        "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
        "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
        "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import col, concat_ws\n",
        "df2 = df.withColumn(\"languagesAtSchool\",\n",
        "   concat_ws(\",\",col(\"languagesAtSchool\")))\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE0_wOoD2DA3",
        "outputId": "cace35f9-3c8c-40bd-c41f-3239311abec6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- languagesAtSchool: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- currentState: string (nullable = true)\n",
            "\n",
            "+----------------+------------------+------------+\n",
            "|name            |languagesAtSchool |currentState|\n",
            "+----------------+------------------+------------+\n",
            "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
            "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
            "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
            "+----------------+------------------+------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- languagesAtSchool: string (nullable = false)\n",
            " |-- currentState: string (nullable = true)\n",
            "\n",
            "+----------------+-----------------+------------+\n",
            "|name            |languagesAtSchool|currentState|\n",
            "+----------------+-----------------+------------+\n",
            "|James,,Smith    |Java,Scala,C++   |CA          |\n",
            "|Michael,Rose,   |Spark,Java,C++   |NJ          |\n",
            "|Robert,,Williams|CSharp,VB        |NV          |\n",
            "+----------------+-----------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"ARRAY_STRING\")\n",
        "spark.sql(\"select name, concat_ws(',',languagesAtSchool) as languagesAtSchool, currentState from ARRAY_STRING\") \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_TjWqgd2olv",
        "outputId": "0a3136d3-dfe7-4667-ffd0-8efdc85510c3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----------------+------------+\n",
            "|name            |languagesAtSchool|currentState|\n",
            "+----------------+-----------------+------------+\n",
            "|James,,Smith    |Java,Scala,C++   |CA          |\n",
            "|Michael,Rose,   |Spark,Java,C++   |NJ          |\n",
            "|Robert,,Williams|CSharp,VB        |NV          |\n",
            "+----------------+-----------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
        "broadcastStates = spark.sparkContext.broadcast(states)\n",
        "\n",
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
        "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
        "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
        "  ]\n",
        "\n",
        "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "def state_convert(code):\n",
        "    return broadcastStates.value[code]\n",
        "\n",
        "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
        "result.show(truncate=False)\n",
        "\n",
        "# Broadcast variable on filter\n",
        "\n",
        "filteDf= df.where((df['state'].isin(broadcastStates.value)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cV0OSxRX3OOU",
        "outputId": "7092bc74-6d4c-4443-cbd3-39d502ad5039"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            "\n",
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|James    |Smith   |USA    |CA   |\n",
            "|Michael  |Rose    |USA    |NY   |\n",
            "|Robert   |Williams|USA    |CA   |\n",
            "|Maria    |Jones   |USA    |FL   |\n",
            "+---------+--------+-------+-----+\n",
            "\n",
            "+---------+--------+-------+----------+\n",
            "|firstname|lastname|country|state     |\n",
            "+---------+--------+-------+----------+\n",
            "|James    |Smith   |USA    |California|\n",
            "|Michael  |Rose    |USA    |New York  |\n",
            "|Robert   |Williams|USA    |California|\n",
            "|Maria    |Jones   |USA    |Florida   |\n",
            "+---------+--------+-------+----------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-bed4e11b4731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Broadcast variable on filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mfilteDf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcastStates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36misin\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_create_column_from_literal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"isin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_create_column_from_literal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"isin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_create_column_from_literal\u001b[0;34m(literal)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_create_column_from_literal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliteral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliteral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.sql.functions.lit.\n: java.lang.RuntimeException: Unsupported literal type class java.util.HashMap {FL=Florida, NY=New York, CA=California}\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.literalTypeUnsupportedError(QueryExecutionErrors.scala:182)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.$anonfun$create$2(literals.scala:173)\n\tat scala.util.Failure.getOrElse(Try.scala:222)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.create(literals.scala:173)\n\tat org.apache.spark.sql.functions$.typedlit(functions.scala:142)\n\tat org.apache.spark.sql.functions$.typedLit(functions.scala:125)\n\tat org.apache.spark.sql.functions$.lit(functions.scala:115)\n\tat org.apache.spark.sql.functions.lit(functions.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bEBI-Fh_4MqD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}